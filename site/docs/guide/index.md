!{"type": "docs", "title": "Lezer system guide"}

# System guide

This is the guide to the lezer parser system. It provides a prose
description of the system's functionality. For the item-by-item
documentation of its interface, see the [reference
manual](/docs/ref/).

## Overview

Lezer is a parser system. Given a formal description of a grammar, it
can produce a set of parse tables. Such tables provide a description
that the parser system can use to efficiently construct a syntax tree
for a given piece of text, describing the structure of the text in
terms of the grammar.

The tables are generated by the [lezer-generator](##generator)
tool, a build tool that takes a file in the format described [later in
this guide](#writing-a-grammar), and output a big, largely unreadable
blob of JavaScript that represents the parse tables. This is something
that happens offline, as part of the build process for a grammar
package.

The [lezer](##lezer) package provides the run-time parsing system.
Combined with a parser built by [lezer-generator](##generator),
it gives you a [`parse`](##lezer.Parser.parse) method that takes a
source file and returns a tree.

These trees, represented by data structures from the
[lezer-tree](##tree) package, are more limited than the
_abstract_ syntax trees you might have seen in other contexts. They
are not very abstract. Each node only stores a type, a start and end
position, and a flat sequence of children. When writing a grammar, you
choose which productions are stored as nodes—the others don't appear
in the tree at all.

This means that the tree is very in memory and cheap to build. It does
make doing refined analysis on it somewhat awkward. The use case
guiding the design of this library is an [editor
system](https://codemirror.net), which keeps a syntax tree of the
edited document, and uses it for things like syntax highlighting and
smart indentation.

To support this use case the parser has a few other interesting
properties. It can be used _incrementally_, meaning it can efficiently
re-parse a document that is slightly changed compared to some previous
version given the parse for the old version. And it has _error
recovery_ built in, meaning that even if the input doesn't adhere to
the grammar, it can still produce some reasonable syntax tree for it.

## Parse Algorithm

Lezer is based on the [LR](https://en.wikipedia.org/wiki/LR_parser)
parser, an algorithm invented by Donald Knuth in 1965, which by
pre-analysing the grammar can derive fully deterministic (and thus
efficient) parsers for _some_ grammars.

Roughly, this algorithm [abstractly
interprets](https://en.wikipedia.org/wiki/Abstract_interpretation) the
grammar, recording the various states the parser can be in, and for
each state, creating a table that holds an entry for each terminal
symbol (token) what it should do next when it sees that token in this
state. If there's no single action to take for a given token in a
given state, the grammar can not be parsed with this algorithm. Such
problems are usually called “shift-reduce” or “reduce-reduce”
conflicts. More about that in a moment.

When writing grammars for LR-based tools, it can help to have a rough
feel for this algorithm. The
[Wikipedia](https://en.wikipedia.org/wiki/Abstract_interpretation)
article linked above is a good introduction. For a more in-depth
treatment, I recommend Chapter 9 of [this book
(PDF)](https://dickgrune.com/Books/PTAPG_1st_Edition/BookBody.pdf).

### Ambiguity

A lot of grammars are either impossible or extremely awkward to
represent as a plain LR grammar. If an element's syntactic role only
becomes clear later in the parse (for example JavaScript's `(x = 1) +
1` versus `(x = 1) => x`, where `(x = 1)` can either be an expression
or a parameter list), plain LR often isn't very practical.

[GLR](https://en.wikipedia.org/wiki/GLR_parser) is an extension of the
parsing algorithm that allows a parse to ‘split’ at an ambiguous point
by applying more than one action for a given token. The alternative
parses then continue beside each other. When a parse can't make any
more progress, it is dropped. As long as at least one parse continues,
we can get our syntax tree.

GLR can be extremely helpful when dealing with local ambiguities, but
when applied naively they can easily lead to an explosion of
concurrent parses and, when the grammar is actually ambiguous,
multiple parses continuing indefinitely so that you're in effect
parsing the document multiple times at once. This completely ruins the
properties that made LR parsing useful: predictability and efficiency.

Lezer allows GLR parsing but requires you to explicitly annotate your
ambiguities in your grammar, so that you can use it to solve problems
that were otherwise difficult, but it won't accidentally start
happening all over your grammar.

### Error Recovery

Though the parser has a [`strict`](##lezer.ParseOptions.strict) mode,
by default it'll proceed through any text, no matter how badly it fits
the grammar, and come up with a tree at the end.

To do this it uses the GLR mechanism to try various recovery tricks
(ignoring the current token or skipping ahead to a place that matches
the current token) alongside each other to see which one, a few tokens
later, gets the best result.

Ignored tokens _are_ added to the tree, wrapped in an error node.
Similarly, the place where the parser skipped ahead is marked with an
error node.

### Incremental Parsing

In order to avoid re-doing work, the parser allows you to provide a
[cache](##lezer.ParseOptions.cache) of nodes, which is a previous tree
mapped to the current document with the [`applyChanges`
method](##tree.Tree.applyChanges). The parser will, when possible,
reuse nodes from this cache rather than re-parsing the parts of the
document they cover.

Because the syntax tree represents sequences of matches (specified in
the grammar notation with the `+` and `*` operators) as balanced
sub-trees, the cost of re-matching unchanged parts of the document is
low, and you can quickly create a new tree even for a huge document.

This isn't bulletproof, though—even a tiny document change, if it
changes the meaning of the stuff that comes after it, can require a
big part of the document to be re-parsed. An example would be adding
or removing a block comment opening marker.

### Contextual Tokenizing

In classical parsing techniques there is a strict separation between
the _tokenizer_, the thing that splits the input into a sequence of
atomic tokens, and the parser.

This separation can be problematic, though. Sometimes, the meaning of
a piece of text depends on context, such as with the ambiguity between
JavaScripts regular expression notation and its division operator
syntax. At other times, a sub-language used in the grammar (say, the
content of a string) has a different concept of what a token is than
the rest of the grammar.

Lezer supports contextual token reading. It will allow the tokens you
declare to overlap (to match the same input) as long as such tokens
can't occur in the same place anywhere in the grammar.

You can also define external tokenizers, which cause the parser to
call out to your code to read a token. Such tokenizers will, again,
only be called when the tokens they produce apply at the current
position.

Even whitespace, the type of tokens implicitly skipped by the parser,
is contextual, and you can have different rules skip different things.

### Grammar Nesting

In some cases, such as JavaScript embedded in HTML or code snippets
inside a literal programming notation, you want to make another parser
responsible for parsing pieces of a document. Lezer supports one form
of this: If the nested syntax has a clearly identifiable end token,
you can specify that anything up to that token should be parsed by
another grammar.

This means that parse trees can contain node types from different
grammars.

## Writing a Grammar

## Running the Parser

## Working with Trees